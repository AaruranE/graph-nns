{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code her is taken from this repository (https://github.com/tkipf/pygcn/tree/master/pygcn), from one of the original authors of the paper: \n",
    "```\n",
    "@article{kipf2016semi,\n",
    "  title={Semi-Supervised Classification with Graph Convolutional Networks},\n",
    "  author={Kipf, Thomas N and Welling, Max},\n",
    "  journal={arXiv preprint arXiv:1609.02907},\n",
    "  year={2016}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from pygcn.layers import GraphConvolution\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# replacing argparse with defaults\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.3203 acc_train: 0.9500 loss_val: 0.7213 acc_val: 0.7867 time: 0.0263s\n",
      "Epoch: 0002 loss_train: 0.3063 acc_train: 0.9714 loss_val: 0.7270 acc_val: 0.7800 time: 0.0192s\n",
      "Epoch: 0003 loss_train: 0.3508 acc_train: 0.9571 loss_val: 0.7763 acc_val: 0.7767 time: 0.0162s\n",
      "Epoch: 0004 loss_train: 0.3323 acc_train: 0.9357 loss_val: 0.7639 acc_val: 0.7700 time: 0.0126s\n",
      "Epoch: 0005 loss_train: 0.3620 acc_train: 0.9500 loss_val: 0.7130 acc_val: 0.8033 time: 0.0118s\n",
      "Epoch: 0006 loss_train: 0.3083 acc_train: 0.9643 loss_val: 0.7581 acc_val: 0.7867 time: 0.0096s\n",
      "Epoch: 0007 loss_train: 0.3424 acc_train: 0.9786 loss_val: 0.7552 acc_val: 0.7800 time: 0.0096s\n",
      "Epoch: 0008 loss_train: 0.3332 acc_train: 0.9643 loss_val: 0.7637 acc_val: 0.7867 time: 0.0097s\n",
      "Epoch: 0009 loss_train: 0.3254 acc_train: 0.9714 loss_val: 0.7295 acc_val: 0.7767 time: 0.0096s\n",
      "Epoch: 0010 loss_train: 0.3271 acc_train: 0.9500 loss_val: 0.8064 acc_val: 0.7667 time: 0.0102s\n",
      "Epoch: 0011 loss_train: 0.3334 acc_train: 0.9429 loss_val: 0.7685 acc_val: 0.7600 time: 0.0098s\n",
      "Epoch: 0012 loss_train: 0.3323 acc_train: 0.9643 loss_val: 0.7220 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0013 loss_train: 0.3484 acc_train: 0.9571 loss_val: 0.7791 acc_val: 0.7767 time: 0.0097s\n",
      "Epoch: 0014 loss_train: 0.3321 acc_train: 0.9571 loss_val: 0.8093 acc_val: 0.7533 time: 0.0095s\n",
      "Epoch: 0015 loss_train: 0.3415 acc_train: 0.9571 loss_val: 0.7886 acc_val: 0.7667 time: 0.0101s\n",
      "Epoch: 0016 loss_train: 0.3381 acc_train: 0.9571 loss_val: 0.7717 acc_val: 0.7800 time: 0.0096s\n",
      "Epoch: 0017 loss_train: 0.3309 acc_train: 0.9500 loss_val: 0.7940 acc_val: 0.7700 time: 0.0098s\n",
      "Epoch: 0018 loss_train: 0.3149 acc_train: 0.9714 loss_val: 0.7066 acc_val: 0.8033 time: 0.0101s\n",
      "Epoch: 0019 loss_train: 0.3185 acc_train: 0.9500 loss_val: 0.7562 acc_val: 0.7800 time: 0.0096s\n",
      "Epoch: 0020 loss_train: 0.3181 acc_train: 0.9643 loss_val: 0.7401 acc_val: 0.8100 time: 0.0178s\n",
      "Epoch: 0021 loss_train: 0.2983 acc_train: 0.9857 loss_val: 0.7364 acc_val: 0.7733 time: 0.0132s\n",
      "Epoch: 0022 loss_train: 0.3124 acc_train: 0.9714 loss_val: 0.7576 acc_val: 0.7733 time: 0.0129s\n",
      "Epoch: 0023 loss_train: 0.3091 acc_train: 0.9500 loss_val: 0.7439 acc_val: 0.7867 time: 0.0098s\n",
      "Epoch: 0024 loss_train: 0.3005 acc_train: 0.9571 loss_val: 0.7162 acc_val: 0.7933 time: 0.0093s\n",
      "Epoch: 0025 loss_train: 0.2838 acc_train: 0.9786 loss_val: 0.7743 acc_val: 0.7700 time: 0.0097s\n",
      "Epoch: 0026 loss_train: 0.3542 acc_train: 0.9357 loss_val: 0.7598 acc_val: 0.7867 time: 0.0096s\n",
      "Epoch: 0027 loss_train: 0.3190 acc_train: 0.9571 loss_val: 0.7736 acc_val: 0.7667 time: 0.0104s\n",
      "Epoch: 0028 loss_train: 0.2805 acc_train: 0.9714 loss_val: 0.7181 acc_val: 0.8100 time: 0.0095s\n",
      "Epoch: 0029 loss_train: 0.3048 acc_train: 0.9714 loss_val: 0.7261 acc_val: 0.7933 time: 0.0103s\n",
      "Epoch: 0030 loss_train: 0.3160 acc_train: 0.9571 loss_val: 0.7867 acc_val: 0.7533 time: 0.0102s\n",
      "Epoch: 0031 loss_train: 0.2679 acc_train: 0.9786 loss_val: 0.7114 acc_val: 0.7867 time: 0.0131s\n",
      "Epoch: 0032 loss_train: 0.3714 acc_train: 0.9214 loss_val: 0.8388 acc_val: 0.7667 time: 0.0095s\n",
      "Epoch: 0033 loss_train: 0.2804 acc_train: 0.9643 loss_val: 0.7583 acc_val: 0.7667 time: 0.0100s\n",
      "Epoch: 0034 loss_train: 0.2864 acc_train: 0.9571 loss_val: 0.7224 acc_val: 0.7867 time: 0.0096s\n",
      "Epoch: 0035 loss_train: 0.2921 acc_train: 0.9643 loss_val: 0.7221 acc_val: 0.7867 time: 0.0106s\n",
      "Epoch: 0036 loss_train: 0.3294 acc_train: 0.9429 loss_val: 0.7765 acc_val: 0.7600 time: 0.0099s\n",
      "Epoch: 0037 loss_train: 0.3327 acc_train: 0.9500 loss_val: 0.7961 acc_val: 0.7767 time: 0.0096s\n",
      "Epoch: 0038 loss_train: 0.2960 acc_train: 0.9643 loss_val: 0.7638 acc_val: 0.7733 time: 0.0102s\n",
      "Epoch: 0039 loss_train: 0.3098 acc_train: 0.9429 loss_val: 0.7762 acc_val: 0.8067 time: 0.0097s\n",
      "Epoch: 0040 loss_train: 0.3309 acc_train: 0.9429 loss_val: 0.7380 acc_val: 0.7967 time: 0.0121s\n",
      "Epoch: 0041 loss_train: 0.2946 acc_train: 0.9857 loss_val: 0.7571 acc_val: 0.7667 time: 0.0100s\n",
      "Epoch: 0042 loss_train: 0.2807 acc_train: 0.9571 loss_val: 0.7535 acc_val: 0.7400 time: 0.0116s\n",
      "Epoch: 0043 loss_train: 0.2904 acc_train: 0.9786 loss_val: 0.7612 acc_val: 0.7667 time: 0.0136s\n",
      "Epoch: 0044 loss_train: 0.3136 acc_train: 0.9571 loss_val: 0.7992 acc_val: 0.7567 time: 0.0114s\n",
      "Epoch: 0045 loss_train: 0.3286 acc_train: 0.9643 loss_val: 0.8128 acc_val: 0.7567 time: 0.0097s\n",
      "Epoch: 0046 loss_train: 0.2980 acc_train: 0.9571 loss_val: 0.6934 acc_val: 0.7900 time: 0.0096s\n",
      "Epoch: 0047 loss_train: 0.3109 acc_train: 0.9429 loss_val: 0.7133 acc_val: 0.7633 time: 0.0105s\n",
      "Epoch: 0048 loss_train: 0.2914 acc_train: 0.9714 loss_val: 0.7108 acc_val: 0.7800 time: 0.0096s\n",
      "Epoch: 0049 loss_train: 0.2860 acc_train: 0.9714 loss_val: 0.6967 acc_val: 0.7833 time: 0.0094s\n",
      "Epoch: 0050 loss_train: 0.3193 acc_train: 0.9643 loss_val: 0.7425 acc_val: 0.7833 time: 0.0100s\n",
      "Epoch: 0051 loss_train: 0.2734 acc_train: 0.9643 loss_val: 0.7433 acc_val: 0.7733 time: 0.0096s\n",
      "Epoch: 0052 loss_train: 0.2820 acc_train: 0.9786 loss_val: 0.7276 acc_val: 0.7700 time: 0.0108s\n",
      "Epoch: 0053 loss_train: 0.2605 acc_train: 0.9857 loss_val: 0.7309 acc_val: 0.7767 time: 0.0099s\n",
      "Epoch: 0054 loss_train: 0.3048 acc_train: 0.9643 loss_val: 0.7349 acc_val: 0.7900 time: 0.0095s\n",
      "Epoch: 0055 loss_train: 0.2980 acc_train: 0.9500 loss_val: 0.7482 acc_val: 0.7633 time: 0.0099s\n",
      "Epoch: 0056 loss_train: 0.2958 acc_train: 0.9500 loss_val: 0.7436 acc_val: 0.7667 time: 0.0095s\n",
      "Epoch: 0057 loss_train: 0.2769 acc_train: 0.9571 loss_val: 0.7380 acc_val: 0.7800 time: 0.0103s\n",
      "Epoch: 0058 loss_train: 0.2898 acc_train: 0.9571 loss_val: 0.7521 acc_val: 0.7767 time: 0.0099s\n",
      "Epoch: 0059 loss_train: 0.2939 acc_train: 0.9643 loss_val: 0.7787 acc_val: 0.7567 time: 0.0094s\n",
      "Epoch: 0060 loss_train: 0.2473 acc_train: 0.9786 loss_val: 0.7655 acc_val: 0.7700 time: 0.0144s\n",
      "Epoch: 0061 loss_train: 0.3061 acc_train: 0.9643 loss_val: 0.7248 acc_val: 0.7767 time: 0.0130s\n",
      "Epoch: 0062 loss_train: 0.3253 acc_train: 0.9714 loss_val: 0.7470 acc_val: 0.7833 time: 0.0145s\n",
      "Epoch: 0063 loss_train: 0.2971 acc_train: 0.9714 loss_val: 0.7437 acc_val: 0.7533 time: 0.0095s\n",
      "Epoch: 0064 loss_train: 0.2647 acc_train: 0.9857 loss_val: 0.7266 acc_val: 0.7667 time: 0.0105s\n",
      "Epoch: 0065 loss_train: 0.2796 acc_train: 0.9643 loss_val: 0.6825 acc_val: 0.7967 time: 0.0103s\n",
      "Epoch: 0066 loss_train: 0.2757 acc_train: 0.9714 loss_val: 0.7407 acc_val: 0.7867 time: 0.0095s\n",
      "Epoch: 0067 loss_train: 0.2969 acc_train: 0.9429 loss_val: 0.7425 acc_val: 0.7833 time: 0.0097s\n",
      "Epoch: 0068 loss_train: 0.3008 acc_train: 0.9571 loss_val: 0.7917 acc_val: 0.7733 time: 0.0095s\n",
      "Epoch: 0069 loss_train: 0.3247 acc_train: 0.9357 loss_val: 0.7295 acc_val: 0.7800 time: 0.0105s\n",
      "Epoch: 0070 loss_train: 0.2974 acc_train: 0.9571 loss_val: 0.7467 acc_val: 0.7733 time: 0.0103s\n",
      "Epoch: 0071 loss_train: 0.2869 acc_train: 0.9714 loss_val: 0.7110 acc_val: 0.7900 time: 0.0096s\n",
      "Epoch: 0072 loss_train: 0.2533 acc_train: 0.9929 loss_val: 0.7896 acc_val: 0.7467 time: 0.0098s\n",
      "Epoch: 0073 loss_train: 0.2948 acc_train: 0.9643 loss_val: 0.7261 acc_val: 0.7933 time: 0.0096s\n",
      "Epoch: 0074 loss_train: 0.2867 acc_train: 0.9643 loss_val: 0.7052 acc_val: 0.8033 time: 0.0104s\n",
      "Epoch: 0075 loss_train: 0.2754 acc_train: 0.9786 loss_val: 0.7366 acc_val: 0.7700 time: 0.0101s\n",
      "Epoch: 0076 loss_train: 0.2809 acc_train: 0.9714 loss_val: 0.6985 acc_val: 0.7733 time: 0.0096s\n",
      "Epoch: 0077 loss_train: 0.2802 acc_train: 0.9643 loss_val: 0.7000 acc_val: 0.8033 time: 0.0099s\n",
      "Epoch: 0078 loss_train: 0.3160 acc_train: 0.9714 loss_val: 0.7690 acc_val: 0.7567 time: 0.0095s\n",
      "Epoch: 0079 loss_train: 0.2517 acc_train: 0.9714 loss_val: 0.7149 acc_val: 0.8033 time: 0.0101s\n",
      "Epoch: 0080 loss_train: 0.3018 acc_train: 0.9643 loss_val: 0.7167 acc_val: 0.7667 time: 0.0113s\n",
      "Epoch: 0081 loss_train: 0.2810 acc_train: 0.9500 loss_val: 0.7177 acc_val: 0.7767 time: 0.0119s\n",
      "Epoch: 0082 loss_train: 0.2763 acc_train: 0.9571 loss_val: 0.7518 acc_val: 0.7833 time: 0.0158s\n",
      "Epoch: 0083 loss_train: 0.2835 acc_train: 0.9786 loss_val: 0.7698 acc_val: 0.7867 time: 0.0110s\n",
      "Epoch: 0084 loss_train: 0.3034 acc_train: 0.9429 loss_val: 0.7006 acc_val: 0.7933 time: 0.0097s\n",
      "Epoch: 0085 loss_train: 0.2938 acc_train: 0.9429 loss_val: 0.6730 acc_val: 0.7900 time: 0.0096s\n",
      "Epoch: 0086 loss_train: 0.2923 acc_train: 0.9643 loss_val: 0.6985 acc_val: 0.8000 time: 0.0106s\n",
      "Epoch: 0087 loss_train: 0.3304 acc_train: 0.9429 loss_val: 0.7670 acc_val: 0.7533 time: 0.0098s\n",
      "Epoch: 0088 loss_train: 0.2992 acc_train: 0.9357 loss_val: 0.7318 acc_val: 0.7733 time: 0.0098s\n",
      "Epoch: 0089 loss_train: 0.2971 acc_train: 0.9714 loss_val: 0.7473 acc_val: 0.8133 time: 0.0098s\n",
      "Epoch: 0090 loss_train: 0.2757 acc_train: 0.9357 loss_val: 0.7658 acc_val: 0.7600 time: 0.0095s\n",
      "Epoch: 0091 loss_train: 0.2960 acc_train: 0.9714 loss_val: 0.7355 acc_val: 0.7833 time: 0.0104s\n",
      "Epoch: 0092 loss_train: 0.2569 acc_train: 0.9643 loss_val: 0.7226 acc_val: 0.7900 time: 0.0096s\n",
      "Epoch: 0093 loss_train: 0.2813 acc_train: 0.9571 loss_val: 0.6890 acc_val: 0.7933 time: 0.0102s\n",
      "Epoch: 0094 loss_train: 0.2597 acc_train: 0.9643 loss_val: 0.7320 acc_val: 0.7867 time: 0.0096s\n",
      "Epoch: 0095 loss_train: 0.2870 acc_train: 0.9500 loss_val: 0.7577 acc_val: 0.7767 time: 0.0100s\n",
      "Epoch: 0096 loss_train: 0.2838 acc_train: 0.9429 loss_val: 0.7726 acc_val: 0.7867 time: 0.0106s\n",
      "Epoch: 0097 loss_train: 0.2496 acc_train: 0.9857 loss_val: 0.7728 acc_val: 0.7800 time: 0.0105s\n",
      "Epoch: 0098 loss_train: 0.2302 acc_train: 0.9786 loss_val: 0.7233 acc_val: 0.7967 time: 0.0096s\n",
      "Epoch: 0099 loss_train: 0.2971 acc_train: 0.9714 loss_val: 0.8078 acc_val: 0.7633 time: 0.0097s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 loss_train: 0.3018 acc_train: 0.9643 loss_val: 0.7368 acc_val: 0.7900 time: 0.0138s\n",
      "Epoch: 0101 loss_train: 0.2722 acc_train: 0.9571 loss_val: 0.7347 acc_val: 0.7767 time: 0.0171s\n",
      "Epoch: 0102 loss_train: 0.2577 acc_train: 0.9714 loss_val: 0.7020 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0103 loss_train: 0.2807 acc_train: 0.9571 loss_val: 0.7638 acc_val: 0.7533 time: 0.0110s\n",
      "Epoch: 0104 loss_train: 0.3027 acc_train: 0.9643 loss_val: 0.7779 acc_val: 0.7600 time: 0.0099s\n",
      "Epoch: 0105 loss_train: 0.2867 acc_train: 0.9714 loss_val: 0.7142 acc_val: 0.7933 time: 0.0097s\n",
      "Epoch: 0106 loss_train: 0.2770 acc_train: 0.9786 loss_val: 0.7045 acc_val: 0.7800 time: 0.0102s\n",
      "Epoch: 0107 loss_train: 0.2720 acc_train: 0.9857 loss_val: 0.7575 acc_val: 0.7800 time: 0.0096s\n",
      "Epoch: 0108 loss_train: 0.2630 acc_train: 0.9571 loss_val: 0.7100 acc_val: 0.7700 time: 0.0097s\n",
      "Epoch: 0109 loss_train: 0.3097 acc_train: 0.9643 loss_val: 0.7689 acc_val: 0.7467 time: 0.0097s\n",
      "Epoch: 0110 loss_train: 0.2558 acc_train: 0.9857 loss_val: 0.7754 acc_val: 0.7833 time: 0.0096s\n",
      "Epoch: 0111 loss_train: 0.2441 acc_train: 0.9714 loss_val: 0.7453 acc_val: 0.7733 time: 0.0102s\n",
      "Epoch: 0112 loss_train: 0.2627 acc_train: 0.9786 loss_val: 0.7051 acc_val: 0.7933 time: 0.0095s\n",
      "Epoch: 0113 loss_train: 0.2847 acc_train: 0.9571 loss_val: 0.7063 acc_val: 0.7667 time: 0.0144s\n",
      "Epoch: 0114 loss_train: 0.2862 acc_train: 0.9714 loss_val: 0.7005 acc_val: 0.7900 time: 0.0112s\n",
      "Epoch: 0115 loss_train: 0.2590 acc_train: 0.9786 loss_val: 0.6900 acc_val: 0.7933 time: 0.0103s\n",
      "Epoch: 0116 loss_train: 0.2601 acc_train: 0.9714 loss_val: 0.7592 acc_val: 0.7633 time: 0.0096s\n",
      "Epoch: 0117 loss_train: 0.2436 acc_train: 0.9786 loss_val: 0.7142 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0118 loss_train: 0.2915 acc_train: 0.9643 loss_val: 0.7549 acc_val: 0.7667 time: 0.0096s\n",
      "Epoch: 0119 loss_train: 0.2441 acc_train: 0.9857 loss_val: 0.7123 acc_val: 0.7667 time: 0.0142s\n",
      "Epoch: 0120 loss_train: 0.3129 acc_train: 0.9429 loss_val: 0.8315 acc_val: 0.7467 time: 0.0221s\n",
      "Epoch: 0121 loss_train: 0.2855 acc_train: 0.9571 loss_val: 0.7642 acc_val: 0.7900 time: 0.0170s\n",
      "Epoch: 0122 loss_train: 0.2611 acc_train: 0.9857 loss_val: 0.7006 acc_val: 0.7833 time: 0.0101s\n",
      "Epoch: 0123 loss_train: 0.2619 acc_train: 0.9929 loss_val: 0.7192 acc_val: 0.7767 time: 0.0129s\n",
      "Epoch: 0124 loss_train: 0.2701 acc_train: 0.9643 loss_val: 0.7318 acc_val: 0.7633 time: 0.0097s\n",
      "Epoch: 0125 loss_train: 0.2236 acc_train: 0.9857 loss_val: 0.7628 acc_val: 0.7800 time: 0.0096s\n",
      "Epoch: 0126 loss_train: 0.2763 acc_train: 0.9643 loss_val: 0.7614 acc_val: 0.7900 time: 0.0102s\n",
      "Epoch: 0127 loss_train: 0.2325 acc_train: 0.9929 loss_val: 0.7283 acc_val: 0.8033 time: 0.0097s\n",
      "Epoch: 0128 loss_train: 0.2531 acc_train: 0.9643 loss_val: 0.7221 acc_val: 0.7800 time: 0.0100s\n",
      "Epoch: 0129 loss_train: 0.2704 acc_train: 0.9786 loss_val: 0.7382 acc_val: 0.7667 time: 0.0098s\n",
      "Epoch: 0130 loss_train: 0.2545 acc_train: 0.9571 loss_val: 0.7112 acc_val: 0.7833 time: 0.0095s\n",
      "Epoch: 0131 loss_train: 0.2831 acc_train: 0.9857 loss_val: 0.7395 acc_val: 0.7633 time: 0.0101s\n",
      "Epoch: 0132 loss_train: 0.2741 acc_train: 0.9571 loss_val: 0.7233 acc_val: 0.7867 time: 0.0096s\n",
      "Epoch: 0133 loss_train: 0.2709 acc_train: 0.9714 loss_val: 0.7252 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0134 loss_train: 0.3013 acc_train: 0.9429 loss_val: 0.7212 acc_val: 0.7867 time: 0.0098s\n",
      "Epoch: 0135 loss_train: 0.2388 acc_train: 0.9786 loss_val: 0.7288 acc_val: 0.7700 time: 0.0096s\n",
      "Epoch: 0136 loss_train: 0.2586 acc_train: 0.9714 loss_val: 0.6918 acc_val: 0.8000 time: 0.0101s\n",
      "Epoch: 0137 loss_train: 0.2601 acc_train: 0.9571 loss_val: 0.7183 acc_val: 0.7867 time: 0.0105s\n",
      "Epoch: 0138 loss_train: 0.2905 acc_train: 0.9500 loss_val: 0.7243 acc_val: 0.7567 time: 0.0095s\n",
      "Epoch: 0139 loss_train: 0.2859 acc_train: 0.9571 loss_val: 0.7344 acc_val: 0.7567 time: 0.0143s\n",
      "Epoch: 0140 loss_train: 0.2860 acc_train: 0.9643 loss_val: 0.7320 acc_val: 0.7667 time: 0.0145s\n",
      "Epoch: 0141 loss_train: 0.2659 acc_train: 0.9643 loss_val: 0.7382 acc_val: 0.7900 time: 0.0097s\n",
      "Epoch: 0142 loss_train: 0.2214 acc_train: 0.9857 loss_val: 0.7503 acc_val: 0.7867 time: 0.0102s\n",
      "Epoch: 0143 loss_train: 0.2616 acc_train: 0.9857 loss_val: 0.7164 acc_val: 0.8067 time: 0.0097s\n",
      "Epoch: 0144 loss_train: 0.2396 acc_train: 0.9857 loss_val: 0.7371 acc_val: 0.7900 time: 0.0096s\n",
      "Epoch: 0145 loss_train: 0.2555 acc_train: 0.9571 loss_val: 0.7138 acc_val: 0.7900 time: 0.0105s\n",
      "Epoch: 0146 loss_train: 0.2641 acc_train: 0.9643 loss_val: 0.7343 acc_val: 0.7633 time: 0.0096s\n",
      "Epoch: 0147 loss_train: 0.2800 acc_train: 0.9786 loss_val: 0.6803 acc_val: 0.7900 time: 0.0098s\n",
      "Epoch: 0148 loss_train: 0.2258 acc_train: 0.9786 loss_val: 0.7173 acc_val: 0.8000 time: 0.0097s\n",
      "Epoch: 0149 loss_train: 0.2468 acc_train: 0.9714 loss_val: 0.6903 acc_val: 0.7833 time: 0.0096s\n",
      "Epoch: 0150 loss_train: 0.2196 acc_train: 0.9786 loss_val: 0.6995 acc_val: 0.7700 time: 0.0105s\n",
      "Epoch: 0151 loss_train: 0.2993 acc_train: 0.9429 loss_val: 0.7400 acc_val: 0.7800 time: 0.0103s\n",
      "Epoch: 0152 loss_train: 0.2653 acc_train: 0.9643 loss_val: 0.7043 acc_val: 0.8000 time: 0.0098s\n",
      "Epoch: 0153 loss_train: 0.3055 acc_train: 0.9500 loss_val: 0.7480 acc_val: 0.7967 time: 0.0101s\n",
      "Epoch: 0154 loss_train: 0.2865 acc_train: 0.9500 loss_val: 0.7277 acc_val: 0.7800 time: 0.0097s\n",
      "Epoch: 0155 loss_train: 0.2230 acc_train: 0.9929 loss_val: 0.6980 acc_val: 0.7900 time: 0.0098s\n",
      "Epoch: 0156 loss_train: 0.2388 acc_train: 0.9857 loss_val: 0.6810 acc_val: 0.7833 time: 0.0102s\n",
      "Epoch: 0157 loss_train: 0.2626 acc_train: 0.9571 loss_val: 0.7225 acc_val: 0.7900 time: 0.0130s\n",
      "Epoch: 0158 loss_train: 0.2389 acc_train: 0.9500 loss_val: 0.7090 acc_val: 0.7767 time: 0.0113s\n",
      "Epoch: 0159 loss_train: 0.2190 acc_train: 1.0000 loss_val: 0.7261 acc_val: 0.7767 time: 0.0137s\n",
      "Epoch: 0160 loss_train: 0.2553 acc_train: 0.9714 loss_val: 0.7081 acc_val: 0.7900 time: 0.0103s\n",
      "Epoch: 0161 loss_train: 0.2297 acc_train: 0.9857 loss_val: 0.7136 acc_val: 0.8000 time: 0.0096s\n",
      "Epoch: 0162 loss_train: 0.2351 acc_train: 0.9786 loss_val: 0.7189 acc_val: 0.8067 time: 0.0099s\n",
      "Epoch: 0163 loss_train: 0.2407 acc_train: 0.9571 loss_val: 0.7167 acc_val: 0.7867 time: 0.0097s\n",
      "Epoch: 0164 loss_train: 0.2565 acc_train: 0.9714 loss_val: 0.7653 acc_val: 0.7800 time: 0.0102s\n",
      "Epoch: 0165 loss_train: 0.2516 acc_train: 0.9786 loss_val: 0.7404 acc_val: 0.7933 time: 0.0101s\n",
      "Epoch: 0166 loss_train: 0.2357 acc_train: 0.9643 loss_val: 0.7087 acc_val: 0.8067 time: 0.0096s\n",
      "Epoch: 0167 loss_train: 0.2466 acc_train: 0.9571 loss_val: 0.6589 acc_val: 0.7867 time: 0.0101s\n",
      "Epoch: 0168 loss_train: 0.2607 acc_train: 0.9786 loss_val: 0.7762 acc_val: 0.7567 time: 0.0094s\n",
      "Epoch: 0169 loss_train: 0.2379 acc_train: 0.9714 loss_val: 0.7326 acc_val: 0.7733 time: 0.0100s\n",
      "Epoch: 0170 loss_train: 0.2516 acc_train: 0.9857 loss_val: 0.7231 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0171 loss_train: 0.2455 acc_train: 0.9643 loss_val: 0.7141 acc_val: 0.7833 time: 0.0099s\n",
      "Epoch: 0172 loss_train: 0.2457 acc_train: 0.9857 loss_val: 0.7179 acc_val: 0.8000 time: 0.0106s\n",
      "Epoch: 0173 loss_train: 0.2153 acc_train: 0.9786 loss_val: 0.7411 acc_val: 0.7867 time: 0.0097s\n",
      "Epoch: 0174 loss_train: 0.2471 acc_train: 0.9643 loss_val: 0.7651 acc_val: 0.7633 time: 0.0111s\n",
      "Epoch: 0175 loss_train: 0.2398 acc_train: 0.9786 loss_val: 0.7717 acc_val: 0.7733 time: 0.0124s\n",
      "Epoch: 0176 loss_train: 0.2502 acc_train: 0.9857 loss_val: 0.7327 acc_val: 0.7733 time: 0.0108s\n",
      "Epoch: 0177 loss_train: 0.2515 acc_train: 0.9714 loss_val: 0.7363 acc_val: 0.7867 time: 0.0107s\n",
      "Epoch: 0178 loss_train: 0.2678 acc_train: 0.9714 loss_val: 0.7315 acc_val: 0.7933 time: 0.0123s\n",
      "Epoch: 0179 loss_train: 0.2545 acc_train: 0.9643 loss_val: 0.7462 acc_val: 0.7467 time: 0.0156s\n",
      "Epoch: 0180 loss_train: 0.2535 acc_train: 0.9643 loss_val: 0.7344 acc_val: 0.7900 time: 0.0104s\n",
      "Epoch: 0181 loss_train: 0.2626 acc_train: 0.9786 loss_val: 0.7731 acc_val: 0.7800 time: 0.0100s\n",
      "Epoch: 0182 loss_train: 0.2657 acc_train: 0.9786 loss_val: 0.7708 acc_val: 0.8033 time: 0.0106s\n",
      "Epoch: 0183 loss_train: 0.2289 acc_train: 0.9714 loss_val: 0.7512 acc_val: 0.7767 time: 0.0103s\n",
      "Epoch: 0184 loss_train: 0.2460 acc_train: 0.9643 loss_val: 0.6967 acc_val: 0.7833 time: 0.0100s\n",
      "Epoch: 0185 loss_train: 0.2851 acc_train: 0.9571 loss_val: 0.7348 acc_val: 0.7700 time: 0.0102s\n",
      "Epoch: 0186 loss_train: 0.2341 acc_train: 0.9786 loss_val: 0.7008 acc_val: 0.7900 time: 0.0101s\n",
      "Epoch: 0187 loss_train: 0.2550 acc_train: 0.9786 loss_val: 0.7423 acc_val: 0.7833 time: 0.0100s\n",
      "Epoch: 0188 loss_train: 0.2609 acc_train: 0.9714 loss_val: 0.7105 acc_val: 0.8000 time: 0.0099s\n",
      "Epoch: 0189 loss_train: 0.2615 acc_train: 0.9714 loss_val: 0.7400 acc_val: 0.7733 time: 0.0108s\n",
      "Epoch: 0190 loss_train: 0.2472 acc_train: 0.9857 loss_val: 0.7782 acc_val: 0.7633 time: 0.0101s\n",
      "Epoch: 0191 loss_train: 0.2355 acc_train: 0.9643 loss_val: 0.7508 acc_val: 0.7700 time: 0.0100s\n",
      "Epoch: 0192 loss_train: 0.2450 acc_train: 0.9857 loss_val: 0.6640 acc_val: 0.8133 time: 0.0105s\n",
      "Epoch: 0193 loss_train: 0.2483 acc_train: 0.9571 loss_val: 0.7841 acc_val: 0.7500 time: 0.0117s\n",
      "Epoch: 0194 loss_train: 0.2277 acc_train: 0.9857 loss_val: 0.7131 acc_val: 0.7967 time: 0.0156s\n",
      "Epoch: 0195 loss_train: 0.2303 acc_train: 0.9714 loss_val: 0.6968 acc_val: 0.7867 time: 0.0098s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0196 loss_train: 0.2392 acc_train: 0.9857 loss_val: 0.7034 acc_val: 0.7967 time: 0.0122s\n",
      "Epoch: 0197 loss_train: 0.2441 acc_train: 0.9643 loss_val: 0.7384 acc_val: 0.7767 time: 0.0122s\n",
      "Epoch: 0198 loss_train: 0.2361 acc_train: 0.9786 loss_val: 0.7447 acc_val: 0.7767 time: 0.0136s\n",
      "Epoch: 0199 loss_train: 0.2326 acc_train: 0.9714 loss_val: 0.7125 acc_val: 0.7867 time: 0.0101s\n",
      "Epoch: 0200 loss_train: 0.2204 acc_train: 0.9714 loss_val: 0.7422 acc_val: 0.7700 time: 0.0111s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.2060s\n",
      "Test set results: loss= 0.6175 accuracy= 0.8280\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 0: Parameter containing:\n",
      "tensor([[-0.1123,  0.0660,  0.0483,  ..., -0.1961,  0.0372, -0.0574],\n",
      "        [-0.0229, -0.0583, -0.1448,  ...,  0.0900,  0.0104, -0.0082],\n",
      "        [ 0.3076, -0.1665,  0.0559,  ..., -0.1334, -0.1438,  0.3124],\n",
      "        ...,\n",
      "        [ 0.2490, -0.1269,  0.1377,  ...,  0.1136, -0.1434,  0.1918],\n",
      "        [ 0.1527, -0.2491, -0.0172,  ..., -0.0866,  0.0041,  0.2658],\n",
      "        [-0.0747,  0.0845,  0.0492,  ...,  0.0774,  0.0809, -0.0845]],\n",
      "       requires_grad=True)\n",
      "Parameter 1: Parameter containing:\n",
      "tensor([ 0.0883,  0.1602,  0.1791,  0.1063,  0.0942,  0.1657, -0.0002,  0.0788,\n",
      "         0.1441,  0.1221,  0.1817,  0.1099,  0.1804,  0.1133,  0.1158,  0.1184],\n",
      "       requires_grad=True)\n",
      "Parameter 2: Parameter containing:\n",
      "tensor([[ 2.5628e+00, -2.5031e+00, -2.4827e+00,  2.5081e+00,  2.3554e+00,\n",
      "         -1.2427e+00, -2.6722e+00],\n",
      "        [-1.8962e+00,  2.2970e+00, -1.3407e+00, -2.4984e+00, -2.0550e+00,\n",
      "          2.4408e+00, -1.6600e+00],\n",
      "        [-1.6479e+00,  1.2116e+00, -1.2976e+00,  1.4544e+00, -2.2317e+00,\n",
      "          1.8901e+00, -1.4615e+00],\n",
      "        [ 2.4926e+00, -1.9602e-01, -2.4995e+00, -2.3059e+00,  2.0695e+00,\n",
      "         -2.5378e+00,  1.7645e+00],\n",
      "        [-2.5233e+00,  2.6831e+00, -2.2583e+00, -2.0799e+00,  2.8282e+00,\n",
      "         -2.5972e+00,  2.2106e+00],\n",
      "        [ 2.5169e+00, -2.4802e+00, -2.0280e+00,  1.8888e-01, -2.1723e+00,\n",
      "          2.5707e+00,  1.2952e+00],\n",
      "        [ 5.6156e-12, -2.3365e-11, -3.4353e-12, -4.6084e-12, -4.7317e-12,\n",
      "         -2.1841e-11, -6.3202e-12],\n",
      "        [-2.4010e+00, -1.1795e+00,  2.7129e+00,  2.7745e+00, -2.6678e+00,\n",
      "         -2.8026e+00,  2.4680e+00],\n",
      "        [-1.8127e+00, -2.6837e+00, -2.3769e+00,  1.4754e+00, -2.6267e+00,\n",
      "          2.4302e+00,  2.6336e+00],\n",
      "        [-2.6103e+00, -2.5392e+00,  2.7045e+00, -2.7444e+00,  2.5702e+00,\n",
      "          2.4956e+00, -1.8868e+00],\n",
      "        [-2.0788e+00,  1.8652e+00, -2.3561e+00,  2.2620e+00, -2.1209e+00,\n",
      "          1.9816e+00, -1.8338e+00],\n",
      "        [-1.8829e+00, -1.8870e+00, -1.2496e+00,  2.2863e+00,  2.1942e+00,\n",
      "          2.3758e+00, -1.5476e+00],\n",
      "        [ 9.5437e-02,  2.1981e+00, -1.7350e+00, -2.5669e+00, -2.1739e+00,\n",
      "          1.6469e+00, -9.5402e-01],\n",
      "        [-3.3842e+00,  2.5760e+00,  3.0208e+00,  1.6585e+00,  1.4074e+00,\n",
      "         -3.6430e+00, -2.8678e+00],\n",
      "        [ 2.5584e+00,  2.4059e+00,  2.4160e+00, -2.5649e+00, -1.8828e+00,\n",
      "         -2.0972e+00, -1.8412e+00],\n",
      "        [ 2.6192e+00, -2.6595e+00,  1.8943e+00,  2.1965e+00, -2.7213e+00,\n",
      "         -1.9047e+00,  2.0767e+00]], requires_grad=True)\n",
      "Parameter 3: Parameter containing:\n",
      "tensor([ 0.4182,  0.3812, -0.0737, -0.3039,  0.6154, -0.3520, -0.0437],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(model.parameters()):\n",
    "    print(f\"Parameter {i}: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaruran/miniconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/aaruran/miniconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GraphConvolution. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model-0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('model-0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m1, m2 in zip(model.parameters(), model2.parameters()):\n",
    "    if not torch.allclose(m1, m2):\n",
    "        print('Found two parameters with notable difference, saving/loading failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that it is possible to save and reload these GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1433, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "for p in params:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23063])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_params = torch.cat([p.reshape(-1) for p in params])\n",
    "cat_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gc1.weight',\n",
       "              tensor([[-0.1123,  0.0660,  0.0483,  ..., -0.1961,  0.0372, -0.0574],\n",
       "                      [-0.0229, -0.0583, -0.1448,  ...,  0.0900,  0.0104, -0.0082],\n",
       "                      [ 0.3076, -0.1665,  0.0559,  ..., -0.1334, -0.1438,  0.3124],\n",
       "                      ...,\n",
       "                      [ 0.2490, -0.1269,  0.1377,  ...,  0.1136, -0.1434,  0.1918],\n",
       "                      [ 0.1527, -0.2491, -0.0172,  ..., -0.0866,  0.0041,  0.2658],\n",
       "                      [-0.0747,  0.0845,  0.0492,  ...,  0.0774,  0.0809, -0.0845]])),\n",
       "             ('gc1.bias',\n",
       "              tensor([ 0.0883,  0.1602,  0.1791,  0.1063,  0.0942,  0.1657, -0.0002,  0.0788,\n",
       "                       0.1441,  0.1221,  0.1817,  0.1099,  0.1804,  0.1133,  0.1158,  0.1184])),\n",
       "             ('gc2.weight',\n",
       "              tensor([[ 2.5628e+00, -2.5031e+00, -2.4827e+00,  2.5081e+00,  2.3554e+00,\n",
       "                       -1.2427e+00, -2.6722e+00],\n",
       "                      [-1.8962e+00,  2.2970e+00, -1.3407e+00, -2.4984e+00, -2.0550e+00,\n",
       "                        2.4408e+00, -1.6600e+00],\n",
       "                      [-1.6479e+00,  1.2116e+00, -1.2976e+00,  1.4544e+00, -2.2317e+00,\n",
       "                        1.8901e+00, -1.4615e+00],\n",
       "                      [ 2.4926e+00, -1.9602e-01, -2.4995e+00, -2.3059e+00,  2.0695e+00,\n",
       "                       -2.5378e+00,  1.7645e+00],\n",
       "                      [-2.5233e+00,  2.6831e+00, -2.2583e+00, -2.0799e+00,  2.8282e+00,\n",
       "                       -2.5972e+00,  2.2106e+00],\n",
       "                      [ 2.5169e+00, -2.4802e+00, -2.0280e+00,  1.8888e-01, -2.1723e+00,\n",
       "                        2.5707e+00,  1.2952e+00],\n",
       "                      [ 5.6156e-12, -2.3365e-11, -3.4353e-12, -4.6084e-12, -4.7317e-12,\n",
       "                       -2.1841e-11, -6.3202e-12],\n",
       "                      [-2.4010e+00, -1.1795e+00,  2.7129e+00,  2.7745e+00, -2.6678e+00,\n",
       "                       -2.8026e+00,  2.4680e+00],\n",
       "                      [-1.8127e+00, -2.6837e+00, -2.3769e+00,  1.4754e+00, -2.6267e+00,\n",
       "                        2.4302e+00,  2.6336e+00],\n",
       "                      [-2.6103e+00, -2.5392e+00,  2.7045e+00, -2.7444e+00,  2.5702e+00,\n",
       "                        2.4956e+00, -1.8868e+00],\n",
       "                      [-2.0788e+00,  1.8652e+00, -2.3561e+00,  2.2620e+00, -2.1209e+00,\n",
       "                        1.9816e+00, -1.8338e+00],\n",
       "                      [-1.8829e+00, -1.8870e+00, -1.2496e+00,  2.2863e+00,  2.1942e+00,\n",
       "                        2.3758e+00, -1.5476e+00],\n",
       "                      [ 9.5437e-02,  2.1981e+00, -1.7350e+00, -2.5669e+00, -2.1739e+00,\n",
       "                        1.6469e+00, -9.5402e-01],\n",
       "                      [-3.3842e+00,  2.5760e+00,  3.0208e+00,  1.6585e+00,  1.4074e+00,\n",
       "                       -3.6430e+00, -2.8678e+00],\n",
       "                      [ 2.5584e+00,  2.4059e+00,  2.4160e+00, -2.5649e+00, -1.8828e+00,\n",
       "                       -2.0972e+00, -1.8412e+00],\n",
       "                      [ 2.6192e+00, -2.6595e+00,  1.8943e+00,  2.1965e+00, -2.7213e+00,\n",
       "                       -1.9047e+00,  2.0767e+00]])),\n",
       "             ('gc2.bias',\n",
       "              tensor([ 0.4182,  0.3812, -0.0737, -0.3039,  0.6154, -0.3520, -0.0437]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state_dict() will be a useful method for saving important attributes of the model to file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
